{% extends 'main.html' %}


     {% block content %}
    <h1> Project Django to render</h1>
    <h1>Django Learning</h1>
    <p> 
        Django is a widely-used Python web application framework with a "batteries-included" philosophy. 
        The principle behind batteries-included is that the common functionality for building web applications should come with the framework instead of as separate libraries.
    </p>
<H2>
    Data Science and Machine Learning Models to learn
</H2>
     <p>
        (ğŸ­) ğ—©ğ—®ğ—¿ğ—¶ğ—®ğ—¯ğ—¹ğ—² ğ—œğ—ºğ—½ğ—¼ğ—¿ğ˜ğ—®ğ—»ğ—°ğ—² ğ—¼ğ—» ğ—–ğ—¼ğ—¹ğ—¹ğ—¶ğ—»ğ—²ğ—®ğ—¿ ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—²ğ˜€
<br/>
â—Don't trust variable importance from random forest blindly. The variable importance of a feature is increased whenever the model splits on the node.
<br/> When two features are collinear, the variable importance of the features becomes diluted.

â­ The better approach is to remove collinearity with variable selection using Pearson/Spearman correlation, VIF, or Lasso regression. Then, you can use the random forest or any other tree-based models to get the final model and interpret the variable importance of the features.
<br/>
<br/>
(ğŸ®) ğ—¥ğ—®ğ—»ğ—±ğ—¼ğ—º ğ—™ğ—¼ğ—¿ğ—²ğ˜€ğ˜ (ğ—¥ğ—™) ğ—¼ğ—» ğ—–ğ—¼ğ—»ğ˜ğ—¶ğ—»ğ˜‚ğ—¼ğ˜‚ğ˜€ ğ—§ğ—®ğ—¿ğ—´ğ—²ğ˜ ğ—©ğ—®ğ—¿ğ—¶ğ—®ğ—¯ğ—¹ğ—²
<br/>
â—If you are using RF or other tree-based models (e.g. XGboost), be aware that your target prediction will be clipped based on the y range that the model has seen in training.

For instance, suppose that the train_y range is (100, 1000), but the test_y range is (300, 1500). 
The model will never predict a value beyond 1,000!

â­ If you suspect the y-range to be unbounded, consider choosing a linear model such as OLS, Lasso, or dense neural networks.
<br/>
<br/>
(ğŸ¯) ğ—¨ğ˜€ğ—² ğ—¦ğ—¶ğ—ºğ—½ğ˜€ğ—¼ğ—»'ğ˜€ ğ—£ğ—®ğ—¿ğ—®ğ—±ğ—¼ğ˜… ğ˜ğ—¼ ğ—¶ğ—ºğ—½ğ—¿ğ—¼ğ˜ƒğ—² ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—ºğ—¼ğ—±ğ—²ğ—¹
<br/>
â—If your model is underperforming the benchmark, don't just add more signals and/or parameters to search in hyperparameter tuning. Do EDA on the residuals of the model. For instance, the global accuracy of your model might be 0.85%, but when you segment it by cohorts (e.g. gender, age, product category), your model might perform better or worse based on cohorts.

â­ For the segments that the model is underperforming, conduct EDA to see if there are additional signals you can add to improve it.

This is the depth of ML knowledge that you should consider in practice and for data science interviews.

If you are a candidate preparing for data science and MLE interviews, check out

--
<br/>
<br/>
ğŸ‘‹ Connect with me as I share daily insights and stories about entrepreneurship, data science, and interview prep.
<br/>
<br/>
ğŸš€ follow me more in linkedin for interview preparations and projects in web development.

    </p>
    {% endblock content %}    

    
